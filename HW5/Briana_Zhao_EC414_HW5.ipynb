{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Briana_Zhao_EC414_HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Ja9AxKp0D1"
      },
      "source": [
        "# Homework 5: SVMs and Logistic Regression\r\n",
        "by Sadie Allen and Yousif Khaireddin\r\n",
        "\r\n",
        "**Due date**: March 17, Wednesday by 11:59pm\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZhutmoHqDgu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK_jmezqqFK0"
      },
      "source": [
        "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\r\n",
        "\r\n",
        "To use Google Colab:\r\n",
        "\r\n",
        "1. Download this `ipynb` file.\r\n",
        "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\r\n",
        "3. Upload this file. It will then open in Colab.\r\n",
        "\r\n",
        "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\r\n",
        "\r\n",
        "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\r\n",
        "\r\n",
        "**Submission instructions**: please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by **March 17th at midnight**.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT67fqvXqTyW"
      },
      "source": [
        "## **Question 1:** Binary Classification with SVMs (40pts)\r\n",
        "\r\n",
        "In this problem, you will gain intuition surrounding Support Vector Machines by solving a simple 2D case by hand. \r\n",
        "\r\n",
        "Suppose you have the following dataset:\r\n",
        "\r\n",
        "$ \\vec{x_P} = (2,0)^T, \\vec{x_Q} = (0,4)^T$ With label $y=-1$\r\n",
        "\r\n",
        "$ \\vec{x_R} = (3,3)^T, \\vec{x_S} = (7,4)^T$ With label $y=+1$\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgP7eP7PCGKz"
      },
      "source": [
        "### **1.1**\r\n",
        "Sketch the points in the x1-x2 plane. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaZMXnVhoUdQ"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdDSQczWCSGL"
      },
      "source": [
        "### **1.2** \r\n",
        "Given the hyperplane parameterized by $\\vec{w}=(3,0)^T, b=-3$, sketch it and compute the parameters in canonical form. \r\n",
        "\r\n",
        "Let the hyperplane parameterized by $(\\vec{w}, b)$ linearly separate the dataset $D$. We say $(\\vec{w}, b)$ is in **canonical form** if:\r\n",
        "\r\n",
        "$min_j y_j(\\vec{w}^T\\vec{x_i}+b) = min_j|\\vec{w}^T\\vec{x_i}+b| = 1 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTKnQUOCqkE"
      },
      "source": [
        "**Solution**:\r\n",
        " \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-wn_7oUCtkW"
      },
      "source": [
        "### **1.3**\r\n",
        "Compute the parameters of the hyperplane passing through $\\vec{x_P}$ and $\\vec{x_Q}$ and add it to your picture. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wphd8e5fCw3N"
      },
      "source": [
        "**Solution**:\r\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jic4zmg_Czqh"
      },
      "source": [
        "### **1.4**\r\n",
        "Compute the orthogonal projection of $\\vec{x_R}$ onto the hyperplane in question 1.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4G9gdbhC3Rw"
      },
      "source": [
        "**Solution**:\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpXfEaPgC6OB"
      },
      "source": [
        "### **1.5**\r\n",
        "Compute the parameters of the maximum margin linearly separating hyperplane (SVM hyperplane) in canonical form. Sketch the hyperplane. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSePYpg2DDf5"
      },
      "source": [
        "**Solution**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCDf7M9lDGTV"
      },
      "source": [
        "### **1.6**\r\n",
        "Let $\\vec{w_{SVM}} = -\\alpha_{P}\\vec{x_P} - \\alpha_{Q}\\vec{x_Q} + \\alpha_{R}\\vec{x_R} + \\alpha_{S}\\vec{x_S}$. Hand compute the $\\alpha$ values. Are they unique?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-NA7gweDLAj"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlBs7Hmdq4dQ"
      },
      "source": [
        "## **Question 2:** Logistic Regression (60pts)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LmAoDlZ9KUE"
      },
      "source": [
        "In this problem, we will use an alternate but equivalent formulation of logistic regression than what you have seen in class. Suppose we have a binary classification data set:\r\n",
        "\r\n",
        "$D = \\{(\\vec{x_1}, y_1), (\\vec{x_2}, y_2), .... ,(\\vec{x_n}, y_n)\\}$, where $\\vec{x_i} \\in \\mathbb{R}^d$ and $y_i \\in \\{0,1\\}$. \r\n",
        "\r\n",
        "Recall that the hypothesis/decision rule in a logistic regression model is given by:\r\n",
        "\r\n",
        "$\\hat{y}_w = h_w(\\vec{x}) = \\sigma(\\vec{w}^T\\vec{x})$, where $\\sigma$  is the sigmoid function  $\\sigma(z) = \\frac{1}{1+e^{-z}} = \\frac{e^z}{e^z+1}$\r\n",
        "\r\n",
        "This is a model of $p(y_i=1|\\vec{x_i})$, so we estimate $p(y_i=1|\\vec{x_i})=h_w(\\vec{x_i})$. We classify $\\vec{x_i}$ as 1 if $h_w(\\vec{x_i}) \\geq 0.5$ and $\\vec{x_i}$ as 0 if $h_w(\\vec{x_i}) < 0.5$. Since our dataset is binary, this makes sense because $p(y_i=0|\\vec{x_i}) = 1 - p(y_i=1|\\vec{x_i}) = 1 - h_w(\\vec{x_i})$. (This will come in handy when you write your prediction function.) \r\n",
        "\r\n",
        "The loss function we will use for our 0-1 logistic regression formulation is: \r\n",
        "\r\n",
        "$$l(\\vec{w}) = NLL(\\vec{w}) + \\frac{\\lambda}{2}||\\vec{w}||^2$$\r\n",
        "\r\n",
        "$$\\text{where } NLL(\\vec{w}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i\\log(h_w(\\vec{x_i})) + (1 - y_i)\\log(1 - h_w(\\vec{x_i}))$$\r\n",
        "\r\n",
        "Thus, we attempt to minimize the negative log likelihood (NLL) with respect to $\\vec{w}$ with L2 normalization to prevent $\\vec{w}$ from becoming very large. \r\n",
        "\r\n",
        "The intuition behind this loss function is as follows: if $y_i=0$, the first term inside the sum is $0$, so the only term contributing the the loss is the second term. Remember that $1-h_w(\\vec{x_i}) = p(y_i=0|\\vec{x_i})$. Thus, the larger $p(y_i=0|\\vec{x_i})$ is, the smaller the contribution to the loss will be. The intuition is similar for $y_i=0$. \r\n",
        "\r\n",
        "For more on this loss function and its formulation, see the following resources:\r\n",
        "\r\n",
        "* https://medium.com/30-days-of-machine-learning/day-4-logistic-regression-df9a7a2220cd\r\n",
        "* https://medium.com/@bhardwajprakarsh/negative-log-likelihood-loss-why-do-we-use-it-for-binary-classification-7625f9e3c944\r\n",
        "\r\n",
        "All that is really important for us is to note that like the other formulation of the NLL, this loss function has no closed form solution for minimization with respect to $\\vec{w}$. However, we can solve for the optimal parameter vector $\\vec{w}$ using gradient descent. This is what we will do in this problem. \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83J_Ri89xHV"
      },
      "source": [
        "### **2.1**\r\n",
        "\r\n",
        "In order to perform gradient descent, we must first determine the gradient vector $\\nabla_w l(w)$ as a function of $\\vec{w}$. Compute the derivate $\\nabla_w l(w)$ here. \r\n",
        "\r\n",
        "**Hints:** \r\n",
        "- if $h(x) = f(x) + g(x)$, then $\\nabla_x h(x)$ = $\\nabla_x f(x)$ + $\\nabla_x g(x)$\r\n",
        "\r\n",
        "- $[\\frac{f(x)}{g(x)}]'$ = $\\frac{g(x)f'(x) - f(x)g'(x)}{(g(x))^2}$\r\n",
        "\r\n",
        "- $[f(g(x)]'$ = $f'(g(x))$ * $g(x)'$ \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcuCBKzp-IFI"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syi0bDEEWaFh"
      },
      "source": [
        "### **2.2**\r\n",
        "In this problem, you will implement logistic regression using gradient descent with the gradient you calculated in 2.1\r\n",
        "\r\n",
        "**Data Loading and Scaling**:\r\n",
        "\r\n",
        "In this problem, we will use a logistic regression model to classify wether or not some patients have breast cancer based on data in a study by  Dr. William H. Wolberg, General Surgery Dept at University of Wisconsin. \r\n",
        "\r\n",
        "Luckily this dataset is already found in sklearn and all we have to do is import it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu5YmGee7zG4",
        "outputId": "bc561d1a-eb75-4c65-fc9f-c57c56c3dfde"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from numpy import log, dot, e\r\n",
        "from numpy.random import rand\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report\r\n",
        "from sklearn.datasets import load_breast_cancer\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.datasets import load_breast_cancer\r\n",
        "\r\n",
        "data = load_breast_cancer()\r\n",
        "\r\n",
        "X = data['data']\r\n",
        "Y = data['target']\r\n",
        "#Y[Y == 0] = -1\r\n",
        "\r\n",
        "print(\"X shape\", X.shape, sep='\\t')\r\n",
        "\r\n",
        "print(\"Y shape\", Y.shape, sep='\\t')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape\t(569, 30)\n",
            "Y shape\t(569,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skcgR9ZSXIY5"
      },
      "source": [
        "If we take a look at the features, X, we will notice that all the features vary within ranges that are extremely far from each other. \r\n",
        "\r\n",
        "Some of these features are in the other of 10s, 100s while other are around 0.01 or 0.1.\r\n",
        "\r\n",
        "To correct this, use sklearn's MinMaxScaler to rescale all features between -1 and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_IoVAZ6XEB5"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\r\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9BIcBSTEi4z"
      },
      "source": [
        "#### a. Why is rescaling important for logistic regression models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbqJ20zRE8wj"
      },
      "source": [
        "**Solution**:\r\n",
        "The model will be much more accurate if the features are rescaled to be in a certain range. Since logistic regression models often use gradient descent, if the features vary a lot in value, then each step we take won't be very smooth/consistent. Uneven step size could also cause gradient descent to take longer. Also, since we are using a regularizer, our model will not be accurate if the features vary a lot. The reguralizer identifies the most important features by applying penalties, so it is better if they are rescaled to be in a certain range.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W35me4DX6gR"
      },
      "source": [
        "#### b. Using **train_test_split** from sklearn, seperate the data using a 80, 20 split.\r\n",
        "\r\n",
        "* Make sure to shuffle the dataset using a **random_state** of 42\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpCjkIvyvyhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9cee8b6-c8be-4049-ade9-1711ce558e0e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size = 0.2, random_state = 42)\r\n",
        "\r\n",
        "print(\"xtrain shape:\", xtrain.shape, sep='\\t')\r\n",
        "print(\"xtest shape:\", xtest.shape, '\\n', sep='\\t')\r\n",
        "\r\n",
        "print(\"ytrain shape:\", ytrain.shape, sep='\\t')\r\n",
        "print(\"ytest shape:\", ytest.shape, sep='\\t')\r\n",
        "\r\n",
        "#print(xtrain[6])\r\n",
        "#print(ytrain)\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xtrain shape:\t(455, 30)\n",
            "xtest shape:\t(114, 30)\t\n",
            "\n",
            "ytrain shape:\t(455,)\n",
            "ytest shape:\t(114,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9W2g-rYYHOq"
      },
      "source": [
        "#### c. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWxNwakxYOwH"
      },
      "source": [
        "Build the following functions:\r\n",
        "\r\n",
        "* `sigmoid(z)`: This is the sigmoid function described in the problem above.\r\n",
        "* `loss(X, y, w, lmda=0.001)`: Loss function that takes X, y, w, and $\\lambda$ and returns the value of $l(\\vec{w})$. Note that this function should return a **scalar**.\r\n",
        "* `predict(X, w)`: Predict function that takes data X and parameter vector $\\vec{w}$ and returns the predictions for each data point in X, either 0 or 1. \r\n",
        "* `gradient(X, y, w, lmda=0.001)`: The gradient function that calculates $\\nabla_{\\vec{w}} l(\\vec{w})$ and returns it (make sure this returns a dx1 vector).\r\n",
        "* `fit(X, y, epochs, lr, lmda)`: This function will perform gradient descent for the given number of epochs and return the optimal parameter vector $\\vec{w}$ and the loss values after each step.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZO7rFpoYOVX"
      },
      "source": [
        "def sigmoid(z): \r\n",
        "    sig = 1 / (1 + np.exp(-z))\r\n",
        "    return sig"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Q3dmj5YXQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1a4478-5055-4514-cc4f-864162638895"
      },
      "source": [
        "def loss(X, y, w, lmda=0.001):\r\n",
        "\r\n",
        "    sigmoid_i = sigmoid(np.dot(np.transpose(w),np.transpose(X)))\r\n",
        "    \r\n",
        "    nll = (np.sum((np.transpose(y)*np.log(sigmoid_i)) + ((1-np.transpose(y))*(np.log(1-sigmoid_i)))))\r\n",
        "    nll = nll * (-1 / len(y))\r\n",
        "    reg = (lmda / 2) * np.dot(np.transpose(w),w)\r\n",
        "\r\n",
        "    return nll + reg\r\n",
        "\r\n",
        "w = rand(xtrain.shape[1], 1)\r\n",
        "#print(xtrain.shape, w.shape)\r\n",
        "temp = loss(xtrain, ytrain, w)\r\n",
        "\r\n",
        "print(temp)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.3194403]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FQPGcP6YmwW"
      },
      "source": [
        "def predict(X, w):        \r\n",
        "    predictions = []\r\n",
        "    num_rows, num_cols = X.shape\r\n",
        "\r\n",
        "    for i in range(num_rows):\r\n",
        "      sigmoid_i = sigmoid(np.dot(np.transpose(w),X[i]))\r\n",
        "      if sigmoid_i >= 0.5:\r\n",
        "        predictions.append(1)\r\n",
        "      else:\r\n",
        "        predictions.append(0)\r\n",
        "\r\n",
        "    return predictions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGCpq6L5l0k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085dc470-2895-4f98-f0b6-7c1aff38d262"
      },
      "source": [
        "def gradient(X, y, w, lmda=0.001):\r\n",
        "    n, d = X.shape\r\n",
        "\r\n",
        "    y_hat = sigmoid(np.dot(np.transpose(w),np.transpose(X)))\r\n",
        "\r\n",
        "    grad = (np.dot(np.transpose(X), np.transpose((np.transpose(y) - y_hat))))\r\n",
        "\r\n",
        "    grad = grad * (-1 / n)\r\n",
        "    return grad\r\n",
        "\r\n",
        "w = rand(xtrain.shape[1], 1)\r\n",
        "print(xtrain.shape, w.shape)\r\n",
        "temp = gradient(xtrain, ytrain, w)\r\n",
        "print(temp.shape)\r\n",
        "#print(temp)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(455, 30) (30, 1)\n",
            "(30, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOx97sD3yPzy"
      },
      "source": [
        "def fit(X, y, epochs=200, lr=0.001, lmda=0.001): # default arguments     \r\n",
        "    loss_arr = []\r\n",
        "    w = rand(X.shape[1], 1)\r\n",
        "                \r\n",
        "    for _ in range(epochs):        \r\n",
        "        # Gradient Descent!\r\n",
        "        curr_loss = loss(X, y, w, lmda)\r\n",
        "        \r\n",
        "        loss_arr.append(curr_loss) \r\n",
        "\r\n",
        "        grad = gradient(X, y, w, lmda)\r\n",
        "\r\n",
        "        temp = lr * grad;\r\n",
        "        \r\n",
        "        w = w - temp\r\n",
        "\r\n",
        "    return w, loss_arr"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiqYHOk5YKgj"
      },
      "source": [
        "#### e. Model Evaluation\r\n",
        "\r\n",
        "Call your `fit` function to obtain the optimal $\\vec{w}$ and loss values throughout training. Use hyperparameters `epochs=500`, `lr=0.1` and `lmda=0.01`. Plot the loss values against the number of epochs. Then, call the `predict` function on `xtest` with your optimal $\\vec{w}$ and calculate the CCR on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "PFM_A7aCzrZZ",
        "outputId": "6fbf480c-35b2-4337-b713-0ef4c984b886"
      },
      "source": [
        "w, loss_arr = fit(xtrain, ytrain, epochs=500, lr=0.1, lmda=0.1)\r\n",
        "\r\n",
        "# CODE HERE: Plot the loss values vs epoch number\r\n",
        "x = list(range(1,501,1))\r\n",
        "y = list(loss_arr)\r\n",
        "y = np.reshape(y, [len(y), 1])\r\n",
        "plt.figure(figsize=(7, 6))\r\n",
        "plt.plot(x,y)\r\n",
        "plt.title(\"Loss Values vs. Epoch Number\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQAhElPRoYYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6269aedf-27b3-4e2d-e0bb-8159879249fc"
      },
      "source": [
        "predictions = predict(xtest,w)\r\n",
        "compare = (predictions == ytest)\r\n",
        "map(int, compare)\r\n",
        "total_correct = sum(compare)\r\n",
        "\r\n",
        "   \r\n",
        "CCR = total_correct / len(ytest) \r\n",
        "print(CCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9473684210526315\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}